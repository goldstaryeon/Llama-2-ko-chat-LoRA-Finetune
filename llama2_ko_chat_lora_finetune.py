# -*- coding: utf-8 -*-
"""llama2-ko-chat-lora-finetune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g2A5aLKfQkNAqf_d76sQ_UDpap2QKBZb
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# ! pip install accelerate peft transformers datasets bitsandbytes
# #런타임 다시 시작

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import get_peft_model, LoraConfig
import torch

model_name = "kfkas/Llama-2-ko-7b-Chat"
dataset_name = "mlabonne/guanaco-llama2-1k"

peft_config = LoraConfig(
    lora_alpha=16,
    lora_dropout=0.1,
    r=64,
    bias='none',
    task_type="CAUSAL_LM"
)

compute_dtype = getattr(torch, "float16")

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)

getattr(torch, "float16")

model = AutoModelForCausalLM.from_pretrained(
    "kfkas/Llama-2-ko-7b-Chat",
    quantization_config=bnb_config,
    device_map='auto'
)
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

model.save_pretrained("llama-2-ko-7b-chat-guanaco")



from transformers import AutoModelForCausalLM
from peft import PeftModel, PeftConfig
import torch

peft_model = "/content/llama-2-ko-7b-chat-guanaco"
base_model = AutoModelForCausalLM.from_pretrained(
    "kfkas/Llama-2-ko-7b-Chat",
    quantization_config=bnb_config,
    device_map='auto'
)

model = PeftModel.from_pretrained(base_model, peft_model)
tokenizer = AutoTokenizer.from_pretrained("kfkas/Llama-2-ko-7b-Chat")

model = model.to("cuda")
model.eval()

prompt = "What is Hacktoberfest?"
inputs = tokenizer(f"<s>[INST] {prompt} [/INST]", return_tensors="pt")
with torch.no_grad():
  outputs = model.generate(
      input_ids=inputs["input_ids"].to("cuda"), max_new_tokens=100
  )
  print(
      tokenizer.batch_decode(
          outputs.detach().cpu().numpy(), skip_special_tokens=True
      )[0]
  )

